# Config file
# Currently, only PostgreSQL is supported. Please set to your database details and credentials

database:
  provider: "postgres"
  host: "localhost"
  port: 5432
  database: "example_db"
  user: "postgres"
  password: "postgres"

# py_protodb will generate the code and compiled protobuffer code written to the path specified by `path:`.
# PLEASE NOTE: Currently only python is supported

output:
  path: "./example"
  suffix: "_db"
  package: "example"
  protoc_path: "/usr/local/bin/"

# Working with protobuffers: py_protodb will generate either proto2 or proto3 files. It is important to
# understand that you should specify proto2 when mapping between a relational database where NULL is an valid value and
# is intrinsic to relational normal form, and protobuffers. Using surrogate sequences/serial values as primary keys is
# common. Having a 0, 1, or many relationship requires that a foreign key value be null, not a default value of 0 (for
# the scalar typeint for the serial datatype).

# py_protodb will generate the .proto code in a package called `model` that will be written to the path specified by
# `path:`

proto:
  path: "./example/proto"
  java_package: "com.example"
  go_package: "gitlab.com/gruvy/example"
  objc_prefix: "GRV"
  version: "proto2"

# py_protodb will read all the tables in one or more schema's. When generating the python code, each module will be
# written to a subdirectory with the schema name. So if your output is 'output', then your code will be
# 'output/public/foo.py' and 'output/test_schema/foo.py' respectively. Please note that py_protodb supports tables
# with the same name in different schemas with foreign relationships between them.

generator:

  # A list of tables to exclude from the schemas being read. NOTE that currently, if the excluding tables exist in both
  # schema's then it will be excluded from both.

  schemas: ["public", "test_schema"]

  # NOTE: when using the following two embedding directives, your foreign key relationships MUST BE acyclic

  excluded_tables: ["excluded", "spatial_ref_sys"]

  # Enabling record versioning is recommended. Record versions are key for a distributed system where multiple
  # concurrent clients might be reading records and then at varying points, may or may not update the record. At
  # issue is the staleness of your record after you update it. It is very likely in a distributed system that another
  # update beat you to it. The version is a record scoped counter that guards against update before yours.
  #
  # For those tables that already have a `version` column, py_protodb will automatically generate the SQL and code
  # to guard against if a prior update by making sure the versions are the same. If they are not, the code will throw
  # and error noting the stale version and the client will simply re-read the record, apply the changes, and then update.

  # NOTE: It is recommended that enable record versioning. This will be used to ensure that when a message/struct that
  # has embedded messages is updated, that the generated code properly handles the possibility that the record in the
  # database might have been changed.

  support_record_version: true

  # If you are working from a new schema, or using an ERD tool to manage your schema, you should consider manually
  # adding a `version bigint` column to those tables you want record versioning. In the case that your schema is
  # programmatically generated, or you are using `py_protodb` on an existing schema, you can have `py_protodb`
  # automatically inject the version column into every table that does not already have it defined.

  inject_version_column: true

  # Override the name of the version column here. It must be a bitint

  version_column: version

  # Setting this to true will result in a method generated for any index whose name is prefixed with 'lookup_'. You
  # should only apply this to non-foreign key and primary key indexes those are handled differently by py_protodb. For
  # indexes that you do NOT want generated as accessors, do not append their name with the keyword.

  indexed_lookups: true

  # Setting this to true will result in go-dmap evaluating any CHECK constraint on columns. If the constraint is
  # an IN statement (i.e. a valid value constraint), py_protodb will generate the protobuffer with an enum type whose
  # labels will match the valid values and map accordingly to the field position of the enum.
  #
  #     For example:
  #
  #     CREATE  TABLE "public".workflow (
  #	        workflow_id          bigint GENERATED ALWAYS AS IDENTITY  NOT NULL ,
  #	        task_no              integer  NOT NULL ,
  #	        milestone_no         integer  NOT NULL ,
  #	        project_id           bigint  NOT NULL ,
  #	        change_id            bigint  NOT NULL ,
  #	        description          text  NOT NULL ,
  #	        workflow_type        varchar  NOT NULL ,
  #	        CONSTRAINT pk_workflow PRIMARY KEY ( workflow_id )
  #     );
  #     ALTER TABLE "public".workflow ADD CONSTRAINT check_workflow_type
  #           CHECK ( workflow IN ('UNKNOWN', 'BUSINESS', 'INGEST', 'LABEL', 'TRAIN', 'INFER') );
  #
  # will generate the following enum field
  #
  #   enum WorkflowType {
  #        UNKNOWN = 0;
  #        BUSINESS = 1;
  #        INGEST = 2;
  #        LABEL = 3;
  #        TRAIN = 4;
  #        INFER = 5;
  #    }

  check_constraints_as_enums: true

  # There are occasionally columns that have sensitive values, like a password hash that you do not want as part of
  # the default SELECT (which in turns means they will be absent from the generated INSERT and UPDATE functions/queries).

  excluded_columns:
    -
      table: "test_schema.user"
      columns: ["pword_hash", "geog"]
    -
      table: "foo"
      columns: ["ignore_me"]

  # Extend the generated protobuffer messages. This will generate the message with the `extensions` clause

  extensions:
    -
      table: "test_schema.user"
      extension: "100 to 199"
    -
      table: "public.foo"
      extension: "100 to 110"

  # Custom query mapping. This will generate a function that will return a result map of column/value from the provided
  # query. For UPDATE and DELETE, it will return the operations response. If you have any questions, you can build the
  # example code and review the generated code.

  # py_protodb is designed to support database side-effects that might be a result of a TRIGGER or STORED PROCEDURE. This
  # design results in the pattern that the entire protobuffer will be returned on any custom UPDATE . Remember, the
  # protobuffer is generated by the columns in the table that HAVE NOT been excluded.

  # For custom query mappings that involve a SELECT option, py_protodb will return an array of result sets where the set
  # is a map of interface types keyed by the select column. You will be expected to resolve the casting when accessing
  # the concrete type.
  #     For example:
  #     results, err = GetPasswordHash(db, user.Email)
  #     v := results[0]["pword_hash"]

  mapping:
    -
      table: "test_schema.user"
      queries:
        -
          name: "list_by_user_type"
          query: "SELECT user_id, first_name, last_name FROM test_schema.user WHERE user_type = $userType LIMIT $limit OFFSET $offset"
        -
          name: "update_pword_hash"
          query: "UPDATE test_schema.user SET pword_hash = $pwordHash WHERE email = $email"
        -
          name: "get_pword_hash"
          query: "SELECT pword_hash FROM test_schema.user WHERE email = $email"
        -
          name: "reset_pword_hash"
          query: "UPDATE test_schema.user SET pword_hash = NULL WHERE email = $email"
        -
          name: "disable_user"
          query: "UPDATE test_schema.user SET enabled = false WHERE email = $email"
        -
          name: "enable_user"
          query: "UPDATE test_schema.user SET enabled = true WHERE email = $email"
        -
          name: "delete_user_by_email"
          query: "DELETE FROM test_schema.user WHERE email = $email"
        -
          name: "set_token"
          query: "UPDATE test_schema.user SET user_token = uuid_generate_v4() WHERE user_id = $userId RETURNING user_token"
        -
          name: "find_nearest"
          query: "SELECT user_id, ST_X(geog::geometry) AS lon, ST_Y(geog::geometry) AS lat FROM test_schema.user WHERE ST_DWithin( geog, Geography(ST_MakePoint($lon, $lat)), $radius) AND ST_X(geog::geometry) != 0.0 AND ST_Y(geog::geometry) != 0.0 ORDER BY geog <-> ST_POINT($lon, $lat)::geography"
        -
          name: "set_user_type"
          query: "UPDATE test_schema.user SET user_type = $userType WHERE user_id = $userId"
        -
          name: "get_user_type"
          query: "SELECT user_type, created_on FROM test_schema.user WHERE user_id = $userId"
        -
          name: "get_users_matching_ids"
          query: "SELECT user_id, first_name, last_name FROM test_schema.user WHERE user_id = ANY($userIDs)"
    -
      table: "part"
      queries:
        -
          name: "get_product"
          query: "SELECT p1.* FROM product p1, product_parts pp, part p WHERE p1.product_id = pp.product_id AND pp.part_id = p.part_id AND p.part_name = $partName"


  # py_protodb supports applying transformations to values as they are read and written from the data mapping code to the
  # table. A good use of this would be to convert the geog value from postgis to lat and lng. While you can do custom
  # query mappings to get the lat,lng, this will put the logic in the CRUD. Because py_protodb operates against database
  # SQL operations, it only support transformations between columns that can be legally defined by SQL. This implementation
  # is not terrible sophisticated, it is important to understand that record map for the specified table is defined by
  # the SELECT statement, which are all the columns except those specifically excluded. This means that when you specify
  # a select transform, if those columns do not exist in the table, they will generated in the record map and returned
  # on each read. This means than you can then use those virtual columns generated by the SELECT/READ to then be used
  # on any writes (INSERT/UPDATE). The following demonstrates how to support lat,lon in with a postgis geography column.
  #
  # NOTE: any columns that are referenced in a function must be preceded by a $
  transforms:
    -
      table: "test_schema.user"
      xforms:
        # For the select transform, we need to know the datatype of the product of the transform. This is needed for
        # generating the protobufs
        select:
          -
            column: "lat"
            data_type: "decimal"
            xform: "ST_Y(geog::geometry)"
          -
            column: "lon"
            data_type: "decimal"
            xform: "ST_X(geog::geometry)"
        insert:
          -
            column: "geog"
            data_type: "geography"
            xform: "ST_POINT($lon, $lat)::geography"
        update:
          -
            column: "geog"
            data_type: "geography"
            xform: "ST_POINT($lon, $lat)::geography"
    -
      table: "public.foo"
      xforms:
        select:
          -
            column: "foobar"
            data_type: "integer"
            xform: "1"

